# RAG Agent - Standalone Chatbot Usage Guide

The `rag_agent.py` file is a **portable, reusable RAG chatbot** that you can use anywhere!

## üì¶ What You Get

- ‚úÖ **Standalone Python module** - No web framework needed
- ‚úÖ **CLI interface** - Interactive chat in your terminal
- ‚úÖ **Importable class** - Use in any Python project
- ‚úÖ **Discord/Slack/Telegram ready** - Easy bot integration
- ‚úÖ **Jupyter notebook compatible** - Use in data science workflows
- ‚úÖ **Context7 integration** - Live library documentation (PyTorch, ROS2, Isaac Sim)
- ‚úÖ **Conversation history** - Memory across messages
- ‚úÖ **Source citations** - Know where answers come from

---

## üöÄ Quick Start

### 1. Set Environment Variables

```bash
export OPENAI_API_KEY=sk-your-key-here
export QDRANT_URL=http://localhost:6333  # or your Qdrant Cloud URL
export QDRANT_API_KEY=your-qdrant-key    # optional, if using Qdrant Cloud
```

### 2. Run as CLI Tool

```bash
cd backend
python rag_agent.py
```

You'll see:
```
ü§ñ RAG Chatbot Agent - Physical AI & Humanoid Robotics
===========================================================

üì° Connecting to Qdrant at http://localhost:6333...
‚úÖ Connected!

Type your questions below. Commands:
  /clear    - Clear conversation history
  /export   - Export conversation to JSON
  /quit     - Exit

You: What is Physical AI?
ü§î Thinking...

ü§ñ Assistant: Physical AI refers to...

üìö Sources:
  1. Module 1, Week 1 (relevance: 0.89)
  2. Module 1, Week 2 (relevance: 0.82)
```

---

## üíª Use as Python Module

### Basic Usage

```python
import asyncio
from rag_agent import RAGAgent, RAGConfig

async def main():
    # Load config from environment
    config = RAGConfig.from_env()

    # Or create custom config
    config = RAGConfig(
        openai_api_key="sk-...",
        qdrant_url="http://localhost:6333",
        model="gpt-4o",
        temperature=0.7,
        top_k=5
    )

    # Create agent
    agent = RAGAgent(config)

    # Ask questions
    response = await agent.ask("What is Physical AI?")

    print(response.content)  # Answer
    print(response.sources)  # List of sources with citations

asyncio.run(main())
```

### With Conversation History

```python
agent = RAGAgent(config)

# First question
response1 = await agent.ask("What is reinforcement learning?")
print(response1.content)

# Follow-up (agent remembers context)
response2 = await agent.ask("How is it used in robotics?")
print(response2.content)

# Clear history
agent.clear_history()
```

### Export Conversation

```python
# Get history as JSON
history = agent.get_history()
print(history)

# Export to file
agent.export_conversation("my_conversation.json")
```

---

## ü§ñ Discord Bot Integration

```python
import discord
from rag_agent import RAGAgent, RAGConfig

client = discord.Client()
rag_config = RAGConfig.from_env()
agent = RAGAgent(rag_config)

@client.event
async def on_message(message):
    if message.author == client.user:
        return

    if message.content.startswith("!ask"):
        question = message.content[5:].strip()

        if not question:
            await message.channel.send("Please ask a question!")
            return

        # Get RAG response
        response = await agent.ask(question)

        # Format response
        embed = discord.Embed(
            title="ü§ñ AI Assistant",
            description=response.content,
            color=0x00ff00
        )

        # Add sources
        if response.sources:
            sources_text = "\n".join([
                f"‚Ä¢ {s['module']}, {s['week']}"
                for s in response.sources[:3]
            ])
            embed.add_field(name="üìö Sources", value=sources_text)

        await message.channel.send(embed=embed)

client.run("YOUR_DISCORD_BOT_TOKEN")
```

---

## üí¨ Slack Bot Integration

```python
from slack_bolt.async_app import AsyncApp
from rag_agent import RAGAgent, RAGConfig

app = AsyncApp(token="xoxb-your-token")
agent = RAGAgent(RAGConfig.from_env())

@app.message("ask")
async def handle_ask(message, say):
    question = message['text'].replace("ask", "").strip()

    response = await agent.ask(question)

    # Format sources
    sources_text = "\n".join([
        f"‚Ä¢ {s['module']}, {s['week']} (relevance: {s['relevance_score']:.2f})"
        for s in response.sources[:3]
    ])

    await say({
        "blocks": [
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"*Answer:*\n{response.content}"}
            },
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"*Sources:*\n{sources_text}"}
            }
        ]
    })

if __name__ == "__main__":
    app.start(3000)
```

---

## üìì Jupyter Notebook Usage

```python
# In your notebook
from rag_agent import RAGAgent, RAGConfig

# Create agent
config = RAGConfig(
    openai_api_key="sk-...",
    qdrant_url="http://localhost:6333"
)
agent = RAGAgent(config)

# Ask questions
response = await agent.ask("Explain PyTorch tensors")
display(Markdown(response.content))

# View sources
import pandas as pd
sources_df = pd.DataFrame(response.sources)
sources_df[['module', 'week', 'relevance_score']]
```

---

## üåê Add to Your Own Web API

### FastAPI Example

```python
from fastapi import FastAPI
from pydantic import BaseModel
from rag_agent import RAGAgent, RAGConfig

app = FastAPI()
agent = RAGAgent(RAGConfig.from_env())

class Question(BaseModel):
    question: str

@app.post("/ask")
async def ask_question(q: Question):
    response = await agent.ask(q.question)
    return {
        "answer": response.content,
        "sources": response.sources
    }
```

### Flask Example

```python
from flask import Flask, request, jsonify
from rag_agent import RAGAgent, RAGConfig
import asyncio

app = Flask(__name__)
agent = RAGAgent(RAGConfig.from_env())

@app.route("/ask", methods=["POST"])
def ask():
    question = request.json.get("question")

    # Run async in sync context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    response = loop.run_until_complete(agent.ask(question))
    loop.close()

    return jsonify({
        "answer": response.content,
        "sources": response.sources
    })
```

---

## ‚öôÔ∏è Configuration Options

### RAGConfig Parameters

```python
config = RAGConfig(
    # Required
    openai_api_key="sk-...",          # Your OpenAI API key
    qdrant_url="http://localhost:6333",  # Qdrant server URL

    # Optional
    qdrant_api_key=None,               # Qdrant Cloud API key
    context7_api_key=None,             # Context7 API key
    collection_name="textbook_chunks", # Qdrant collection name
    embedding_dim=1536,                # Embedding dimension
    model="gpt-4o",                    # OpenAI model
    temperature=0.7,                   # Response creativity (0-1)
    max_tokens=1000,                   # Max response length
    top_k=5,                          # Number of chunks to retrieve
    relevance_threshold=0.7,           # Minimum relevance score
    use_context7=True                  # Enable Context7 integration
)
```

---

## üìä Response Format

```python
@dataclass
class Message:
    role: str                    # 'user' or 'assistant'
    content: str                 # The answer text
    sources: List[Dict] = None   # List of source citations

# Source dictionary format:
{
    "type": "textbook",          # or "library_docs"
    "text": "...",              # Chunk text
    "module": "Module 1",        # Textbook module
    "week": "Week 1",            # Week number
    "file": "path/to/file.md",   # Source file
    "relevance_score": 0.89      # Similarity score (0-1)
}
```

---

## üîÑ Migration from Existing Backend

If you're already using the FastAPI backend (`backend/src/services/rag_service.py`), you can switch to this agent:

### Before (FastAPI backend):
```python
from src.services.rag_service import rag_service

result = await rag_service.answer_question(
    question="What is Physical AI?",
    use_library_docs=True
)
```

### After (RAG Agent):
```python
from rag_agent import RAGAgent, RAGConfig

agent = RAGAgent(RAGConfig.from_env())
response = await agent.ask("What is Physical AI?")
```

---

## üéØ Real-World Use Cases

### 1. **Study Assistant CLI Tool**
Students run `python rag_agent.py` to ask questions while studying.

### 2. **Discord Study Server**
Create a Discord bot that answers questions 24/7 in your study server.

### 3. **Jupyter Notebook Research**
Researchers use the agent in notebooks to query textbook content while writing papers.

### 4. **Slack Workspace Bot**
Team collaboration with AI assistant integrated into Slack.

### 5. **WhatsApp Bot**
Use Twilio + RAG Agent to create a WhatsApp study assistant.

### 6. **Telegram Bot**
Deploy on Heroku/Railway with python-telegram-bot library.

---

## üêõ Troubleshooting

### "Collection not found" Error

```bash
# Make sure you've indexed your content first
cd backend
python scripts/index_content.py
```

### "OpenAI API Key not set"

```bash
export OPENAI_API_KEY=sk-your-key-here
```

### Qdrant Connection Error

```bash
# If using local Qdrant
docker run -p 6333:6333 qdrant/qdrant

# Or set URL to Qdrant Cloud
export QDRANT_URL=https://your-cluster.qdrant.io
export QDRANT_API_KEY=your-key
```

---

## üìö Dependencies

Already included in `backend/requirements.txt`:
- `openai` - OpenAI API
- `qdrant-client` - Vector database
- `python-dotenv` - Environment variables

---

## üöÄ Next Steps

1. **Copy `rag_agent.py` to your new project**
2. **Install dependencies**: `pip install openai qdrant-client python-dotenv`
3. **Set environment variables**
4. **Import and use!**

```python
from rag_agent import RAGAgent, RAGConfig

agent = RAGAgent(RAGConfig.from_env())
response = await agent.ask("Your question here")
print(response.content)
```

---

## üìÑ License

MIT License - Use this agent anywhere you want!

---

## üí° Tips

- Use `temperature=0.3` for factual answers, `0.7-0.9` for creative answers
- Increase `top_k` to 10 for more comprehensive answers (slower)
- Set `relevance_threshold=0.8` for stricter source filtering
- Export conversations with `/export` command to analyze later
- Clear history with `/clear` if switching topics

---

Enjoy your portable RAG chatbot! üéâ
